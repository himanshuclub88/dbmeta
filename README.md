---

# ğŸ“˜ MiniDB â€” A Pure Python Metadata Database Engine

### Query Batch Job Metadata Using SQL + Python, Directly From Folder Structures

---

# ğŸŒŸ Overview

**MiniDB** is a pure Python, zero-dependency engine designed specifically for **reading and analyzing batch job metadata** stored in folder structures.
It automatically detects `metadata.json` files inside job-run folders, extracts structured metadata, and turns them into **queryable database tables**.

You get:

* **Python Query API** (chainable, SQL-like)
* **Advanced SQL Engine**
* **Automatic Table Detection**
* **JOIN / MULTI-JOIN**
* **Aggregations (SUM, AVG, MIN, MAX, COUNT)**
* **GROUP BY + HAVING**
* **Complex WHERE (AND / OR / NOT / parentheses)**
* **ORDER BY, LIMIT**
* **Multi-day consolidation**
* **Metadata-only reading (all other files ignored)**

MiniDB turns this:

```
batch_logs/
 â”œâ”€â”€ JobA_20240201_010203/
 â”‚     â”œâ”€â”€ metadata.json
 â”‚     â”œâ”€â”€ execution.log
 â”‚     â””â”€â”€ error.txt
 â”œâ”€â”€ JobA_20240202_020501/
 â”‚     â”œâ”€â”€ metadata.json
 â”‚     â””â”€â”€ stdout.log
 â””â”€â”€ JobA_20240203_030422/
       â””â”€â”€ metadata.json
```

Into:

```
db.execution_info
db.inputs
db.stats
```

Each table contains rows from **all days**, merged automatically.

---

# ğŸ­ Why MiniDB? (Real-World Metadata Motivation)

Batch systems (Airflow, Azure Data Factory, Informatica, TWS, Autosys, Databricks) create new metadata every time a job runs:

```
jobName_timestamp/
    metadata.json
    *.log
    *.txt
    debug/
    temp/
```

Metadata typically contains:

* start_time / end_time
* status: SUCCESS / FAILED
* duration_sec
* files read
* data source
* rows processed
* pipeline stats
* lineage
* retry attempts

Over time, metadata folders grow:

* thousands of runs
* across many days
* across many environments

### â— Pain Points Solved by MiniDB

| Problem                                                 | MiniDB Solution                         |
| ------------------------------------------------------- | --------------------------------------- |
| Metadata scattered across folders                       | Auto-detects all `metadata.json`        |
| Tons of irrelevant files (`*.log`, `*.txt`, other JSON) | Ignores everything except metadata.json |
| Hard to join metadata across runs                       | JOIN / MULTI-JOIN built in              |
| Hard to run analytics                                   | SQL Engine                              |
| Hard to write custom transformations                    | Python Query Engine                     |
| Changing schema                                         | Schema-less, automatically adapts       |

---

# ğŸ“ Metadata Folder Structure

MiniDB expects a structure like:

```
batch_logs/
 â”œâ”€â”€ JobA_20240201_000101/
 â”‚     â””â”€â”€ metadata.json
 â”œâ”€â”€ JobA_20240201_010101/
 â”‚     â””â”€â”€ metadata.json
 â”œâ”€â”€ JobA_20240201_020301/
 â”‚     â””â”€â”€ metadata.json
 ...
```

Inside each folder:

```
metadata.json  â† MiniDB reads ONLY this file
execution.log  â† ignored
error.txt      â† ignored
debug.json     â† ignored
*.tmp          â† ignored
```

---

# ğŸ§° Example metadata.json

```json
{
    "execution_info": {
        "start_time": "2024-02-01T01:00:00Z",
        "end_time": "2024-02-01T01:10:00Z",
        "status": "SUCCESS",
        "duration_sec": 600
    },
    "stats": {
        "rows_in": 500000,
        "rows_out": 499500
    },
    "inputs": {
        "files_read": 12,
        "source": "landing/finance/"
    }
}
```

MiniDB auto-creates 3 tables:

```
db.execution_info
db.stats
db.inputs
```

Each row includes:

```
iid = folder name (e.g., JobA_20240201_000101)
```

---

# âš™ï¸ Instantiating MiniDB

Your modified constructor:

```python
from minibd import FolderDB

db = FolderDB(base_path="batch_logs", base_metadeta="metadata.json")
```

To use a custom metadata filename:

```python
db = FolderDB("batch_logs", base_metadeta="job_metadata.json")
```

MiniDB ignores all other files.

---

# ğŸ“… Multi-Day Metadata Example (Automatic Table Consolidation)

If your metadata folders look like:

```
batch_logs/
 â”œâ”€â”€ 2024-02-01_JobA/
 â”‚     â””â”€â”€ metadata.json
 â”œâ”€â”€ 2024-02-02_JobA/
 â”‚     â””â”€â”€ metadata.json
 â”œâ”€â”€ 2024-02-03_JobA/
 â”‚     â””â”€â”€ metadata.json
```

MiniDB automatically merges them into unified tables.

### âœ” execution_info table:

| iid             | start_time           | status  | duration_sec |
| --------------- | -------------------- | ------- | ------------ |
| 2024-02-01_JobA | 2024-02-01T01:00:00Z | SUCCESS | 600          |
| 2024-02-02_JobA | 2024-02-02T01:00:00Z | FAILED  | 180          |
| 2024-02-03_JobA | 2024-02-03T01:00:00Z | SUCCESS | 520          |

### âœ” stats table:

| iid             | rows_in | rows_out |
| --------------- | ------- | -------- |
| 2024-02-01_JobA | 500000  | 499500   |
| 2024-02-02_JobA | 150000  | 149800   |
| 2024-02-03_JobA | 800000  | 798500   |

### âœ” inputs table:

| iid             | files_read | source           |
| --------------- | ---------- | ---------------- |
| 2024-02-01_JobA | 18         | landing/finance/ |
| 2024-02-02_JobA | 10         | landing/finance/ |
| 2024-02-03_JobA | 22         | landing/finance/ |

This automatic consolidation requires **no extra code** â€” MiniDB handles it.

---

# ğŸ” Python Query API

-     ` .all() ` return in list of json
-     ` .show() ` print on screen in row column format


## WHERE example

```python
db.execution_info.where(
    ("status", "=", "FAILED"),
    ("duration_sec", ">", 300)
).show()
```

## JOIN example

```python
db.execution_info.join(db.stats, on="iid").show()
```

## MULTI JOIN

```python
db.execution_info.multi_join([db.stats, db.inputs], on="iid").show()
```

## GROUP BY + HAVING

```python
db.execution_info \
  .group_by("status") \
  .having(("COUNT", ">", 10)) \
  .show()
```

---

# ğŸ§  SQL Engine (Advanced)

```python
db.sql("YOUR SQL QUERY HERE").show() #print row table
k=db.sql("YOUR SQL QUERY HERE").all() #returns in list of JSON
print(k)
```

## Failed sqls

```sql
db.sql("""
SELECT iid, duration_sec
FROM execution_info
WHERE status = 'FAILED'
""").show()
```

## Full join across metadata categories

```sql
db.sql("""
SELECT e.iid, e.status, s.rows_in, i.files_read
FROM execution_info e
JOIN stats s USING(iid)
JOIN inputs i USING(iid)
""").show()
```

## Grouping example

```sql
db.sql("""
SELECT status, COUNT(*)
FROM execution_info
GROUP BY status
""").show()
```

## Average duration per day

```sql
db.sql("""
SELECT start_time AS Date,
       AVG(duration_sec)
FROM execution_info
GROUP BY day
""").show()
```

---

# ğŸ”’ File Ignoring Behavior

MiniDB **only reads** the metadata file specified by `base_metadeta`.

Ignored files in job folders:

| File Type               | Ignored? |
| ----------------------- | -------- |
| `*.log`                 | âœ”        |
| `*.txt`                 | âœ”        |
| `other-json-files.json` | âœ”        |
| `debug/`                | âœ”        |
| `temp/`                 | âœ”        |
| `*.tmp`                 | âœ”        |

MiniDB remains stable even in noisy production folders.

---

# ğŸ§± Architecture Diagram

```
FolderDB(base_path, base_metadeta)
 â”œâ”€â”€ Scan all job folders
 â”œâ”€â”€ Read ONLY metadata.json
 â”œâ”€â”€ Auto-generate tables (one per top-level key)
 â””â”€â”€ Attach SQLParserAdvanced + TableQuery

TableQuery Engine
 â”œâ”€â”€ WHERE(), SELECT(), LIMIT()
 â”œâ”€â”€ ORDER BY()
 â”œâ”€â”€ JOIN(), MULTI-JOIN()
 â”œâ”€â”€ GROUP BY(), HAVING()
 â””â”€â”€ all()

SQLParserAdvanced Engine
 â”œâ”€â”€ Tokenizer
 â”œâ”€â”€ Boolean/AND/OR/NOT parser
 â”œâ”€â”€ Parentheses support
 â”œâ”€â”€ Aggregation + GROUP BY + HAVING
 â”œâ”€â”€ JOIN USING()
 â”œâ”€â”€ ORDER BY, LIMIT
 â””â”€â”€ Executes via TableQuery
```

---

# ğŸ›£ Roadmap

Planned features:

* CREATE TABLE + INSERT via SQL
* DISTINCT
* Column qualification (`e.status`)
* Subquery support
* Query optimizer
* Parquet/CSV export
* Web dashboard for metadata browsing

---

# ğŸ Conclusion

MiniDB turns **folders of batch metadata** into a **queryable lightweight database**, without:

* Hive
* Spark
* MySQL
* Heavy warehousing tools

It is:

* Fast
* Simple
* Flexible
* Metadata-focused
* Production-friendly
* 100% Python

Perfect for:

* ETL/BATCH job monitoring
* Failure analytics
* Pipeline performance tuning
* Data engineering tooling
* Debugging historical job runs
* Metadata lineage exploration

---# metadb
# dbmeta
# dbmeta
